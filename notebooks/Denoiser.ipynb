{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join,exists\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "\n",
    "\n",
    "from tensorflow import convert_to_tensor\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import  load_model,Model\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Reshape,Input,Dropout, Activation, Dense, GlobalMaxPooling2D,Conv2D,Flatten,MaxPooling2D,InputLayer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import copy\n",
    "\n",
    "from efficientnet.tfkeras import EfficientNetB7\n",
    "from tensorflow.keras import datasets\n",
    "from foolbox.attacks import LinfFastGradientAttack,LinfDeepFoolAttack\n",
    "from foolbox.distances import LpDistance\n",
    "from foolbox.models import TensorFlowModel\n",
    "from foolbox import criteria\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from art.attacks.evasion import SaliencyMapMethod\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from art.attacks.evasion import BoundaryAttack\n",
    "from art.utils import load_dataset #to play with cifar images\n",
    "\n",
    "import graphviz\n",
    "import pydot\n",
    "\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    gpus[0],\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4*1024)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mnist=datasets.mnist.load_data(path='mnist.npz')\n",
    "X_train_mnist,y_train_mnist=data_mnist[0][0],data_mnist[0][1]\n",
    "X_test_mnist,y_test_mnist=data_mnist[1][0],data_mnist[1][1]\n",
    "y_train_mnist = to_categorical(y_train_mnist, 10)\n",
    "y_test_mnist = to_categorical(y_test_mnist, 10)\n",
    "\n",
    "\"\"\" Train \"\"\"\n",
    "X_train_img_mnist = np.full((60000, 32, 32, 3), 0)\n",
    "for i, s in enumerate(X_train_mnist):\n",
    "    X_train_img_mnist[i] = cv2.cvtColor(np.pad(s,2), cv2.COLOR_GRAY2RGB) \n",
    "    \n",
    "\"\"\" Test \"\"\"\n",
    "X_test_img_mnist = np.full((10000, 32, 32, 3), 0)\n",
    "for i, s in enumerate(X_test_mnist):\n",
    "    X_test_img_mnist[i] = cv2.cvtColor(np.pad(s,2), cv2.COLOR_GRAY2RGB) \n",
    "    \n",
    "X_train_mnist=X_train_img_mnist\n",
    "X_test_mnist= X_test_img_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_cifar, y_train_cifar), (X_test_cifar, y_test_cifar), _,_=load_dataset('cifar10')\n",
    "for i, im in enumerate(X_train_cifar):\n",
    "    X_train_cifar[i]=255*im\n",
    "for i, im in enumerate(X_test_cifar):\n",
    "    X_test_cifar[i]=255*im    \n",
    "\n",
    "\n",
    "dict_classes={}\n",
    "dict_classes[0]='airplane'\n",
    "dict_classes[1]='automobile'\n",
    "dict_classes[2]='bird'\n",
    "dict_classes[3]='cat'\n",
    "dict_classes[4]='deer'\n",
    "dict_classes[5]='dog'\n",
    "dict_classes[6]='frog'\n",
    "dict_classes[7]='horse'\n",
    "dict_classes[8]='ship'\n",
    "dict_classes[9]='truck'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test), min, max=load_dataset('cifar10')\n",
    "y_test=list(map(np.argmax,y_test))\n",
    "for i,im in enumerate(X_train):\n",
    "    X_train[i]=255*im\n",
    "    \n",
    "for i,im in enumerate(X_test):\n",
    "    X_test[i]=255*im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classes={}\n",
    "dict_classes[0]='airplane'\n",
    "dict_classes[1]='automobile'\n",
    "dict_classes[2]='bird'\n",
    "dict_classes[3]='cat'\n",
    "dict_classes[4]='deer'\n",
    "dict_classes[5]='dog'\n",
    "dict_classes[6]='frog'\n",
    "dict_classes[7]='horse'\n",
    "dict_classes[8]='ship'\n",
    "dict_classes[9]='truck'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_data_set(name):\n",
    "    \n",
    "    if name=='Mnist':\n",
    "        X_train=X_train_mnist\n",
    "        X_test= X_test_mnist\n",
    "        y_train = y_train_mnist\n",
    "        y_test= y_test_mnist\n",
    "\n",
    "    elif name=='Cifar':\n",
    "        X_train=X_train_cifar\n",
    "        X_test= X_test_cifar\n",
    "        y_train = y_train_cifar\n",
    "        y_test= y_test_cifar    \n",
    "    return(X_train,X_test,y_train,y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_effnet(data_set_name):\n",
    "    (X_train,X_test,y_train,y_test)=pick_data_set(data_set_name)\n",
    "    tf.keras.backend.clear_session()\n",
    "    effnet_base = EfficientNetB7(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "    effnet_base.trainable=True\n",
    "    x = GlobalMaxPooling2D(name='pool_1')(effnet_base.layers[-2].output)\n",
    "    x = Dropout(0.2, name=\"dropout_2\")(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Dense(10,name='fc_2')(x)\n",
    "    o = Activation('softmax', name='act_2')(x)\n",
    "    model_effnet = Model(inputs=effnet_base.input, outputs=[o])\n",
    "\n",
    "\n",
    "    \n",
    "    if exists('/media/hdd1/benchmark_adversarial_attacks/effnet_model_'+str(data_set_name)+'.h5')==False:\n",
    "        model_effnet.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='nadam',\n",
    "            metrics=['accuracy']\n",
    "            )\n",
    "        history = model_effnet.fit(X_train, y_train,\n",
    "                      epochs=5,\n",
    "                      batch_size = 128,\n",
    "                      validation_split=0.1,\n",
    "                      shuffle=True,\n",
    "                      verbose=1)\n",
    "        model_effnet.save('/media/hdd1/benchmark_adversarial_attacks/effnet_model_'+str(data_set_name)+'.h5')\n",
    "\n",
    "    else:\n",
    "        model_effnet=load_model('/media/hdd1/benchmark_adversarial_attacks/effnet_model_'+str(data_set_name)+'.h5')\n",
    "        \n",
    "    return(model_effnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Denoiser creation and data set creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoise(Model):\n",
    "    def __init__(self):\n",
    "        super(Denoise, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "          Input(shape=(32, 32, 3)), \n",
    "          Conv2D(64, (3,3), activation='relu', padding='same', strides=2),\n",
    "          Conv2D(64, (3,3), activation='relu', padding='same', strides=2),\n",
    "          Flatten(),\n",
    "          Dense(4096,activation='relu'),          \n",
    "          Reshape((8,8,64))\n",
    "            ])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "        Conv2DTranspose(64, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "        Conv2DTranspose(128, kernel_size=2, strides=2, activation='relu', padding='same'),\n",
    "        Conv2D(3, kernel_size=(3,3), activation='relu', padding='same')])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_set_maker(model,attack, image_list, labels):\n",
    "    model_to_fool = TensorFlowModel(model, bounds=(0, 255))\n",
    "    success=[]\n",
    "    adv_list=[]\n",
    "    benign_list=[]\n",
    "    adv_true_label=[]\n",
    "    epsilon=[5]\n",
    "    labels=list(map(np.argmax,labels))\n",
    "    print('======epsilon: '+str(epsilon[0])+'======')\n",
    "    for i,image in enumerate(tqdm(image_list,position=0)):\n",
    "        if i!=0 and i%(len(labels)//3)==0:\n",
    "            print('======adv_list_size: '+str(len(adv_list))+'======')\n",
    "            epsilon=[epsilon[0]*1.5]\n",
    "            print('======epsilon: '+str(epsilon[0])+'======')\n",
    "        image = np.asarray(image)[:,:,:3].astype('float32')\n",
    "        image = convert_to_tensor(np.expand_dims(image,axis=0))\n",
    "        label=labels[i]\n",
    "        label = tf.convert_to_tensor(np.array([label]))\n",
    "        _, clipped, is_adv = attack(model_to_fool,image,label,epsilons=epsilon)\n",
    "        if bool(is_adv[0]) ==True:\n",
    "            adv_list.append(np.array(clipped[0][0]))\n",
    "            adv_true_label.append(labels[i]) \n",
    "            benign_list.append(image) \n",
    "            \n",
    "    for i,image in enumerate(benign_list):\n",
    "        benign_list[i]=np.squeeze(image)\n",
    "        \n",
    "    return(list(adv_list),list(benign_list),adv_true_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adv_data_set(data_set_name,model_effnet):\n",
    "    attack_FGSM=LinfFastGradientAttack()\n",
    "    if exists('data/adv images and benign images '+str(data_set_name))==False:\n",
    "        (adv_list,benign_list,adv_true_label)=data_set_maker(model_effnet,attack_FGSM, X_test[:6000], y_test[:6000])\n",
    "        with open('data/adv images and benign images '+str(data_set_name), 'wb') as f:\n",
    "            pickle.Pickler(f).dump(adv_list)\n",
    "            pickle.Pickler(f).dump(benign_list)\n",
    "            pickle.Pickler(f).dump(adv_true_label)\n",
    "    else:\n",
    "        with open('data/adv images and benign images '+str(data_set_name), 'rb') as f:\n",
    "            adv_list=pickle.Unpickler(f).load()\n",
    "            benign_list=pickle.Unpickler(f).load()\n",
    "            adv_true_label=pickle.Unpickler(f).load()\n",
    "\n",
    "    ###let's add some benign examples to the data set and shuffle the result\n",
    "\n",
    "    adv_list.extend(X_test[6000:7000])\n",
    "    benign_list.extend(X_test[6000:7000])\n",
    "    adv_true_label.extend(y_test[6000:7000])\n",
    "\n",
    "    adv_list=np.array(adv_list)\n",
    "    benign_list=np.array(benign_list)\n",
    "    adv_true_label=np.array(adv_true_label)\n",
    "\n",
    "    indices = np.arange(len(adv_list))\n",
    "    random.shuffle(indices)\n",
    "    adv_list = adv_list[indices]\n",
    "    benign_list = benign_list[indices]\n",
    "    adv_true_label = adv_true_label[indices]\n",
    "    return(adv_list,benign_list,adv_true_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's add some non advsersarial images in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Denoiser training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(25,45))\n",
    "# for i,(adv_image, true_label) in enumerate(zip(adv_list[len(adv_list)-1000:len(adv_list)-1000+10],adv_true_label[len(adv_list)-1000:len(adv_list)-1000+10])): \n",
    "   \n",
    "#     i=2*i\n",
    "#     ax = plt.subplot(5, 2, i + 1)\n",
    "#     plt.imshow(adv_image.astype('int32'))\n",
    "#     predicted_adv_label=dict_classes[np.argmax(model_effnet.predict(np.expand_dims(adv_image,axis=0)))]\n",
    "#     true_label=dict_classes[true_label]\n",
    "#     if true_label==predicted_adv_label:\n",
    "#         color='green'\n",
    "#     else:\n",
    "#         color='red'\n",
    "#     plt.title('true label: '+str(true_label)+'\\n predicted_label: '+str(predicted_adv_label),color=color)\n",
    "    \n",
    "#     ax = plt.subplot(5, 2, i + 2)\n",
    "#     new_image=autoencoder.predict(np.expand_dims(adv_image,axis=0))\n",
    "#     predicted_label=dict_classes[np.argmax(model_effnet.predict(new_image))]\n",
    "#     plt.imshow(np.squeeze(new_image).astype('int32'))\n",
    "#     if true_label==predicted_label:\n",
    "#         color='green'\n",
    "#     else:\n",
    "#         color='red'\n",
    "#     plt.title('true label: '+str(true_label)+'\\n predicted_label: '+str(predicted_label),color=color)\n",
    "#     plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the denoiser on top of the effnet_model.\n",
    "Beforehand we made the denoiser not trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the peformances metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DOC (x_adv_list,x_list):\n",
    "    N=len(x_adv_list)\n",
    "    sum=0\n",
    "    for i in range (N):        \n",
    "        sum+=100*np.linalg.norm(np.reshape(x_adv_list[i] - x_list[i], -1),ord=1)/np.linalg.norm(np.reshape(x_list[i], -1),ord=1)\n",
    "    return(sum/N)\n",
    "\n",
    "def succes_rate (success_list):\n",
    "    success=len([i for i in success_list if i==True])\n",
    "    total=len(success_list)\n",
    "    return(success/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Attack robusteness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### White box attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_performances_computer(model_to_attack,predicting_model,attack, image_list, labels,epsilon):\n",
    "    model_to_attack=TensorFlowModel(model_to_attack , bounds=(0, 255))\n",
    "    success_on_attacked_model=[]\n",
    "    success_on_predicting_model=[]\n",
    "    adv_list=[]\n",
    "    labels=list(map(np.argmax,labels))\n",
    "    for i,image in enumerate(tqdm(image_list,position=0)):\n",
    "        image = np.asarray(image)[:,:,:3].astype('float32')\n",
    "        image = convert_to_tensor(np.expand_dims(image,axis=0))\n",
    "        label=labels[i]\n",
    "        label = tf.convert_to_tensor(np.array([label]))\n",
    "        _, clipped, is_adv = attack(model_to_attack,image,label,epsilons=epsilon)\n",
    "        success_on_attacked_model.append(bool(is_adv[0]))\n",
    "        adv_list.append(np.array(clipped[0]))\n",
    "        prediction=predicting_model.predict(np.expand_dims(clipped[0],axis=0))\n",
    "        if np.argmax(prediction)!=labels[i]:\n",
    "            success_on_predicting_model.append(True)\n",
    "        else:\n",
    "            success_on_predicting_model.append(False)\n",
    "            \n",
    "        \n",
    "    DOC_attack=DOC(adv_list,image_list)\n",
    "    SR_on_attacked_model=succes_rate(success_on_attacked_model)\n",
    "    SR_on_predicting_model=succes_rate(success_on_predicting_model)\n",
    "    return(DOC_attack,SR_on_attacked_model,SR_on_predicting_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_runner(model_to_attack,predicting_model,image_list, labels_list, epislons_list):  \n",
    "    \n",
    "    attack_FGSM=LinfFastGradientAttack()\n",
    "    SR_FGSM_dic={}  \n",
    "    SR_FGSM_dic_with_defense={}\n",
    "    for epsilon in epislons_list:\n",
    "        print('======'+'epislon: '+str(epsilon)+'======')\n",
    "        DOC_FGSM,SR_FGSM,SR_FGSM_with_defense=attack_performances_computer(model_to_attack,predicting_model,attack_FGSM, image_list, labels_list,[epsilon])      \n",
    "        SR_FGSM_dic[DOC_FGSM]=SR_FGSM\n",
    "        SR_FGSM_dic_with_defense[DOC_FGSM]=SR_FGSM_with_defense   \n",
    "    return(SR_FGSM_dic,SR_FGSM_dic_with_defense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_run(data_set_name):\n",
    "    print(\"====data set selection====\")\n",
    "    (X_train,X_test,y_train,y_test)=pick_data_set(data_set_name)\n",
    "    print(\"====benign model training====\")\n",
    "    model_effnet=train_and_save_effnet(data_set_name)\n",
    "    print(\"====adversarial data set creation====\")\n",
    "    (adv_list,benign_list,adv_true_label)=make_adv_data_set(data_set_name,model_effnet)\n",
    "    autoencoder = Denoise()\n",
    "    print(\"====Denoiser training====\")\n",
    "    autoencoder.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(),metrics=['MeanSquaredError'])\n",
    "    autoencoder.fit(np.array(adv_list[:len(adv_list)-1000]),np.array(benign_list[:len(adv_list)-1000]),\n",
    "                epochs=30,\n",
    "                batch_size=32,\n",
    "                shuffle=True)\n",
    "    \n",
    "    autoencoder.trainable=False\n",
    "    inputs=Input(shape=(32,32,3))\n",
    "    x=autoencoder(inputs)\n",
    "    o=model_effnet(x)\n",
    "\n",
    "    model_with_defense=Model(inputs,[o])\n",
    "\n",
    "    model_with_defense.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    print(\"====Adversarial attacks on model+denoiser====\")\n",
    "    SR_FGSM_dic_without_defense,SR_FGSM_dic_with_defense=attack_runner(model_effnet,model_with_defense,X_test[7000:7100], y_test[7000:7100], [0.1,0.5,1,5,7,10,15,20,30,50])\n",
    "    if exists('data/denosier_efficiency_white_box_'+str(data_set_name))==False:\n",
    "        with open('data/denosier_efficiency_white_box_'+str(data_set_name), 'wb') as f:\n",
    "                pickle.Pickler(f).dump(SR_FGSM_dic_without_defense)\n",
    "                pickle.Pickler(f).dump(SR_FGSM_dic_with_defense)\n",
    "                \n",
    "    return(SR_FGSM_dic_without_defense,SR_FGSM_dic_with_defense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====data set selection====\n",
      "====benign model training====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0203 19:10:03.159173 140541819410240 deprecation.py:323] From /home/timotheerio/.local/lib/python3.6/site-packages/foolbox/models/tensorflow.py:13: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "  0%|          | 0/6000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====adversarial data set creation====\n",
      "======epsilon: 5======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 249/6000 [02:00<45:22,  2.11it/s]"
     ]
    }
   ],
   "source": [
    "(SR_FGSM_dic_without_defense,SR_FGSM_dic_with_defense)=total_run('Mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(SR_FGSM_dic_with_defense.keys()),list(SR_FGSM_dic_with_defense.values()),label='with denoiser')\n",
    "plt.plot(list(SR_FGSM_dic_without_defense.keys()),list(SR_FGSM_dic_without_defense.values()),label='without denoiser')\n",
    "# plt.xscale('log')\n",
    "plt.grid(True,which=\"both\", linestyle='--')\n",
    "plt.title('Successs rate for different degrees of change \\n on Mnist images',fontsize=11)\n",
    "plt.xlabel('DOC (%)')\n",
    "plt.ylabel('SR')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(SR_FGSM_dic_without_defense,SR_FGSM_dic_with_defense)=total_run('Cifar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(SR_FGSM_dic_with_defense.keys()),list(SR_FGSM_dic_with_defense.values()),label='with denoiser')\n",
    "plt.plot(list(SR_FGSM_dic_without_defense.keys()),list(SR_FGSM_dic_without_defense.values()),label='without denoiser')\n",
    "# plt.xscale('log')\n",
    "plt.grid(True,which=\"both\", linestyle='--')\n",
    "plt.title('Successs rate for different degrees of change \\n on Cifar images',fontsize=11)\n",
    "plt.xlabel('DOC (%)')\n",
    "plt.ylabel('SR')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of the denoiser againt a black box attack (boundary attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_models(data_set_name):\n",
    "    (X_train,X_test,y_train,y_test)=pick_data_set(data_set_name)\n",
    "    model_effnet=train_and_save_effnet(data_set_name)\n",
    "    (adv_list,benign_list,adv_true_label)=make_adv_data_set(data_set_name,model_effnet)\n",
    "    \n",
    "    autoencoder.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(),metrics=['MeanSquaredError'])\n",
    "    autoencoder.fit(np.array(adv_list[:len(adv_list)-1000]),np.array(benign_list[:len(adv_list)-1000]),\n",
    "                epochs=30,\n",
    "                batch_size=32,\n",
    "                shuffle=True)\n",
    "    \n",
    "    autoencoder.trainable=False\n",
    "    inputs=Input(shape=(32,32,3))\n",
    "    x=autoencoder(inputs)\n",
    "    o=model_effnet(x)\n",
    "\n",
    "    model_with_defense=Model(inputs,[o])\n",
    "\n",
    "    model_with_defense.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    return(model_effnet,model_with_defense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model_without_def,model_with_def)=prep_models('Mnist')\n",
    "(X_train,X_test,y_train,y_test)=pick_data_set('Mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TensorFlowV2Classifier(model=model_without_def, input_shape=(32,32,3),clip_values=(0, 255),nb_classes=10)\n",
    "degree_of_change_without_def={}\n",
    "attack = BoundaryAttack(estimator=classifier, targeted=False, max_iter=0, delta=0.001, epsilon=0.01)\n",
    "iter_step =1\n",
    "target=X_test[1]\n",
    "image_list=[]\n",
    "x_adv = None\n",
    "for i in range(100):\n",
    "    x_adv = attack.generate(x=np.array([target]), x_adv_init=x_adv)\n",
    "\n",
    "    #clear_output()\n",
    "    print(\"Adversarial image at step %d.\" % (i * iter_step), \"L2 error\", \n",
    "          np.linalg.norm(np.reshape(x_adv[0] - target, [-1])),\n",
    "          \"and class label %d.\" % np.argmax(classifier.predict(x_adv)[0]))\n",
    "    plt.imshow(x_adv[0][..., ::-1].astype('int32'))\n",
    "    image_list.append(x_adv[0][..., ::-1].astype(np.uint))\n",
    "    plt.show(block=False)\n",
    "    degree_of_change_without_def[i * iter_step]=DOC([x_adv[0]],[target])\n",
    "    \n",
    "    if hasattr(attack, 'curr_delta') and hasattr(attack, 'curr_epsilon'):\n",
    "        attack.max_iter = iter_step \n",
    "        attack.delta = attack.curr_delta\n",
    "        attack.epsilon = attack.curr_epsilon\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TensorFlowV2Classifier(model=model_with_def, input_shape=(32,32,3),clip_values=(0, 255),nb_classes=10)\n",
    "degree_of_change_with_def={}\n",
    "attack = BoundaryAttack(estimator=classifier, targeted=False, max_iter=0, delta=0.001, epsilon=0.01)\n",
    "iter_step =1\n",
    "image_list=[]\n",
    "target=X_test[1]\n",
    "x_adv = None\n",
    "for i in range(100):\n",
    "    x_adv = attack.generate(x=np.array([target]), x_adv_init=x_adv)\n",
    "\n",
    "    #clear_output()\n",
    "    print(\"Adversarial image at step %d.\" % (i * iter_step), \"L2 error\", \n",
    "          np.linalg.norm(np.reshape(x_adv[0] - target, [-1])),\n",
    "          \"and class label %d.\" % np.argmax(classifier.predict(x_adv)[0]))\n",
    "    plt.imshow(x_adv[0][..., ::-1].astype('int32'))\n",
    "    image_list.append(x_adv[0][..., ::-1].astype(np.uint))\n",
    "    plt.show(block=False)\n",
    "    degree_of_change_with_def[i * iter_step]=DOC([x_adv[0]],[target])\n",
    "    \n",
    "    if hasattr(attack, 'curr_delta') and hasattr(attack, 'curr_epsilon'):\n",
    "        attack.max_iter = iter_step \n",
    "        attack.delta = attack.curr_delta\n",
    "        attack.epsilon = attack.curr_epsilon\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(degree_of_change_without_def.keys()),list(degree_of_change_without_def.values()),label='DOC without denoiser')\n",
    "plt.plot(list(degree_of_change_with_def.keys()),list(degree_of_change_with_def.values()),label='DOC with denoiser')\n",
    "plt.grid(True,which=\"both\", linestyle='--')\n",
    "plt.title('DOC of the adversarial image with respect to the number of iterations on EfficientNet', fontsize=8)\n",
    "plt.xlabel('Iterarion')\n",
    "plt.ylabel('DOC (%)')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
